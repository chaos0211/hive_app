# -*- coding: utf-8 -*-
"""
ä»Ž MySQL è¯»å– appstore_rankings_daily â†’ è§„åˆ™æ¸…æ´— â†’ å†™å…¥ Hive åˆ†åŒºè¡¨
åˆ†åŒºï¼šchart_date, brand_id, genre
å†™å…¥æ¨¡å¼ï¼šæŒ‰åˆ†åŒºè¦†ç›–ï¼ˆåŠ¨æ€åˆ†åŒºï¼‰
"""

import os
from pyspark.sql import SparkSession, functions as F, Window
from pyspark.sql.types import DecimalType, IntegerType, LongType, BooleanType, StringType, TimestampType

MYSQL_URL = "jdbc:mysql://127.0.0.1:33309/hive_app?useSSL=false&characterEncoding=utf8&serverTimezone=UTC"
MYSQL_USER = "root"
MYSQL_PWD  = "123456"
MYSQL_TABLE = "appstore_rankings_daily"

HIVE_DB = "hive_app"
HIVE_TABLE = "appstore_rankings_cleaned"

# éœ€è¦æ¸…æ´—çš„æ—¥æœŸèŒƒå›´ï¼ˆå¯é€‰å‚æ•°åŒ–ï¼šç”¨çŽ¯å¢ƒå˜é‡ä¼ å…¥ï¼‰
# ä¸ºç©ºåˆ™å…¨é‡ï¼›å»ºè®®æŒ‰å¤©è·‘ï¼šEXPORT_START=2024-08-01 EXPORT_END=2024-08-31
EXPORT_START = os.environ.get("EXPORT_START")  # 'YYYY-MM-DD'
EXPORT_END   = os.environ.get("EXPORT_END")    # 'YYYY-MM-DD'

def build_spark():
    spark = (
        SparkSession.builder
        .appName("qimai-clean-to-hive")
        .enableHiveSupport()  # ðŸ”‘ ä½¿ç”¨ Hive Metastore
        # è¦†ç›–åˆ†åŒºéœ€è¦
        .config("spark.sql.sources.partitionOverwriteMode", "dynamic")
        .config("spark.sql.hive.convertMetastoreParquet", "false")  # é¿å…æŸäº›çŽ¯å¢ƒåˆ†åŒºè¯†åˆ«é—®é¢˜
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")
    return spark

def read_mysql(spark):
    reader = (
        spark.read
        .format("jdbc")
        .option("url", MYSQL_URL)
        .option("user", MYSQL_USER)
        .option("password", MYSQL_PWD)
        .option("driver", "com.mysql.cj.jdbc.Driver")
        .option("dbtable", MYSQL_TABLE)
        # æ‹‰å¤§ fetchsizeï¼Œå‡å°‘å¾€è¿”
        .option("fetchsize", 2000)
    )
    df = reader.load()
    # é€‰å–æ‰€éœ€åˆ—ï¼ˆä¸Žæ¨¡åž‹ä¸€è‡´ï¼‰
    cols = [
        "chart_date","brand_id","country","device","genre","app_genre",
        "index","ranking","change","is_ad",
        "app_id","app_name","subtitle","icon_url","publisher","publisher_id","price",
        "file_size_bytes","file_size_mb","continuous_first_days",
        "source","raw_json","crawled_at","updated_at"
    ]
    df = df.select(*[c for c in cols if c in df.columns])
    return df

def apply_range_filter(df):
    if EXPORT_START and EXPORT_END:
        return df.filter((F.col("chart_date") >= EXPORT_START) & (F.col("chart_date") <= EXPORT_END))
    return df

def clean_rules(df):
    # ç»Ÿä¸€ç±»åž‹
    df = (
        df
        .withColumn("ranking", F.col("ranking").cast(IntegerType()))
        .withColumn("index", F.col("index").cast(IntegerType()))
        .withColumn("change", F.col("change").cast(IntegerType()))
        .withColumn("is_ad", F.col("is_ad").cast(BooleanType()))
        .withColumn("file_size_bytes", F.col("file_size_bytes").cast(LongType()))
        .withColumn("file_size_mb",
                    F.when(F.col("file_size_bytes").isNotNull(),
                           (F.col("file_size_bytes") / F.lit(1024*1024)).cast(DecimalType(18,2)))
                     .otherwise(None))
        .withColumn("price",
                    F.when(F.col("price").isNull(), F.lit(0.0)).otherwise(F.col("price")).cast(DecimalType(10,2)))
        .withColumn("app_name", F.trim(F.col("app_name").cast(StringType())))
        .withColumn("subtitle", F.trim(F.col("subtitle").cast(StringType())))
        .withColumn("publisher", F.trim(F.col("publisher").cast(StringType())))
        .withColumn("icon_url", F.trim(F.col("icon_url").cast(StringType())))
        .withColumn("app_genre", F.trim(F.col("app_genre").cast(StringType())))
        .withColumn("source", F.when(F.col("source").isNull(), F.lit("qimai")).otherwise(F.col("source")))
    )

    # è¿‡æ»¤æ— æ•ˆ/å¼‚å¸¸æ•°æ®
    df = df.filter(F.col("app_id").isNotNull() & F.col("app_name").isNotNull())
    df = df.filter(F.col("ranking").between(1, 200))
    # ä½“ç§¯æžç«¯è„å€¼è¿‡æ»¤ï¼š> 40GB è§†ä¸ºå¼‚å¸¸ï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰
    df = df.filter((F.col("file_size_bytes").isNull()) | (F.col("file_size_bytes") <= F.lit(40*1024*1024*1024)))

    # åŽ»é‡ï¼šåŒä¸€åˆ†åŒºé”® + index ä¿ç•™ updated_at æœ€æ–°çš„ä¸€æ¡
    w = Window.partitionBy("chart_date","brand_id","genre","country","device","index").orderBy(F.col("updated_at").desc_nulls_last())
    df = df.withColumn("_rn", F.row_number().over(w)).filter(F.col("_rn") == 1).drop("_rn")

    return df

def reorder_for_hive(df):
    # æŒ‰ Hive è¡¨åˆ—é¡ºåº & åˆ†åŒºåˆ—æ”¾æœ€åŽ
    cols = [
        "country","device","app_genre","index","ranking","change","is_ad",
        "app_id","app_name","subtitle","icon_url","publisher","publisher_id","price",
        "file_size_bytes","file_size_mb","continuous_first_days",
        "source","raw_json","crawled_at","updated_at",
        # partitions
        "chart_date","brand_id","genre"
    ]
    return df.select(*cols)

def write_to_hive(df):
    # åˆ†åŒºè¦†ç›–ï¼ˆåªè¦†ç›–æœ¬æ¬¡ DataFrame ä¸­å‡ºçŽ°çš„åˆ†åŒºï¼‰
    (
        df.write
        .mode("overwrite")
        .format("parquet")
        .partitionBy("chart_date","brand_id","genre")
        .saveAsTable(f"{HIVE_DB}.{HIVE_TABLE}")
    )

def main():
    spark = build_spark()
    try:
        df = read_mysql(spark)
        df = apply_range_filter(df)
        df = clean_rules(df)
        df = reorder_for_hive(df)
        write_to_hive(df)
        print(f"âœ… Done. Wrote to {HIVE_DB}.{HIVE_TABLE}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()